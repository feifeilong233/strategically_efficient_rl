simple_push_rnd_self_play:
    run: SELF_PLAY
    stop:
        self_play_rounds: 30
    checkpoint_freq: 500
    checkpoint_at_end: True
    num_samples: 5
    config:
        alg: PPO_CURIOSITY
        # === Evaluation ===
        population:
          - path: populations/simple_push_ppo
            alg: SIMULTANEOUS_PLAY
            mapping: [[1, learned_policy_1]]
        random_eval: True
        multiagent_eval_interval: 20
        # === Self Play ===
        symmetric: False
        self_play_round_stop: 
            training_iteration: 50
        self_play_pretrain_stop:
            training_iteration: 0
        # === Environment ===
        horizon: 200
        env: mpe
        env_config:
            scenario_name: simple_push
            action_space: discrete
        # === Curiosity ===
        model:
            custom_options:
                start_weight: 1.0
                end_weight: 0.0
                exploration_steps: 400000
                burn_in: 8000
                delay: 4000
                decay: 0.02
                curiosity_module: RND
                curiosity_config:
                    scale: 0.5
                    fcnet_activation: elu
                    fcnet_hiddens: [256, 256]
                    fcnet_outputs: 32
                    agent_action: True
                    joint_action: False
        # === Curiosity PPO ===
        intrinsic_lambda: 0.95
        intrinsic_gamma: 0.95
        num_agents: 2
        # === PPO ===
        lambda: 0.95
        gamma: 0.95
        entropy_coeff: 0.001
        clip_param: 0.1
        lr: 0.001
        num_sgd_iter: 8
        train_batch_size: 1600
        rollout_fragment_length: 400
        batch_mode: truncate_episodes